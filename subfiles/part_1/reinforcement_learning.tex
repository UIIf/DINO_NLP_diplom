\documentclass[../part_1.tex]{subfiles}

\begin{document}
\subsubsection{Обучение с подкреплением}
    \label{sec:reinforcement_learning}
    \par Обучение с подкреплением --  это вид машинного обучения, при котором агент(модель) обучается на основе опыта взаимодействия со средой, принимая решения максимизирующие награду.
    \par В отличие от прошлых методов агенты ориентированы на последовательное принятие решений в условиях неопредленности.
    
    \par Основное преимущество обучения с подкреплением:
    \begin{itemize}
        \item \textit{Подходит для задач с отложенной наградой} -- может учитывать долгосрочные последствия действий, а не только мгновенную выгоду.
        \item \textit{Возможность обучения без размеченных данных} -- не требует готовых "правильных ответов".
        \item \textit{Не требует дифференцируемости} - функция оценки не обязана быть дифференцируемой.
    \end{itemize}
    \par Основные недостатки обучения с подкреплением:
    \begin{itemize}
        \item \textit{Сложная оценка} -- необходимо получать внешнюю оценку, которая может требовать значительного времени вычисления(например, симуляции физических процессов) или человеческого вмешательства.
        \item \textit{Проблема исследования-эксплуатации} -- агент должен балансировать между исследованием и эксплуатацией. Исследование -- проба новых действий, для поиска лучшей стратегии. Эксплуатация -- использование уже известных лучших действий. Для получения качественной модели алгоритм обучения должен позволять модели исследовать новые способы решения задачи и эксплуатировать уже изученные.
    \end{itemize}
    \par Пример моделей, которые обучаются с помощью обучения с подкреплением:
    \begin{itemize}
        \item LLM\cite{llm}
        \item Алгоритм автоматического управления транспортных средства.
        \item Игровой искусственный интеллект.
    \end{itemize}
\end{document}