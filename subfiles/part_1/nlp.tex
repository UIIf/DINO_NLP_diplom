\documentclass[../part_1.tex]{subfiles}

\begin{document}
\subsection{Обработка естественного языка}
\par Обработка естественного языка -- направление искусственного интеллекта, объединяющее лингвистику и компьютерные науки для анализа, понимания и генерации человеческого языка. Современные \acrshort{nlp}-системы способны обрабатывать текст не только на уровне слов, но и понимать контекст, иронию и даже культурные отсылки.
\par В задачах обработки естественного языка входные данные по своей природе не имеют жестких ограничений по длине и структуре. Поэтому для эффективной обработки таких данных требуются архитектуры, способные учитывать произвольные зависимости между элементами последовательности. 
\par Чтобы представить последовательность текста в виде последовательности чисел, используют токенизаторы. Токенизатор -- инструмент, который разбивает текст на отдельные элементы. Этот процесс трансформирует человекопонятный текст в машинночитаемые токены. В современных токенизаторах токен может описывать как самые часто встречающиеся слова, отдельные части слов(корни, суффиксы, окончания), так и отдельные символы. 
\par Для того чтобы легче менять значимость токена, используется слой Embedding. В результате работы данного слоя каждому токену сопоставляется вектор, при этом в процессе обучения эти векторы изменяются так, чтобы передавать смысловую нагрузку токенов.
\par В последнее время наиболее популярной архитектурой нейронных сетей является Transformer. В основе этой ахритектуры лежит механизм внимания(Attention), который позволяет модели анализировать взаимосвязи между всеми словами в тексте одновременно, независимо от их позиции. Attention вычисляет взвешанные зависимости между токенами, определяя, насколько каждое слово влияет на другие в последовательности, что обеспечивает контекстное понимание, превосходящее классические модели.
\par Так как Transformer не использует сверточные и рекурентные слои, он не имеет представления о порядке слов. Для учета позиционной информации применяют positional embedding -- специальные векторы, добавляемые к embedding слов перед передачей на вход модели.
\par Для обучения векторов embedding делают предобучение модели. Предобучение -- процесс первоначального обучения нейросети на большом объеме неразмеченных данных с использованием методов \acrfull{ssl}. На этом этапе модель так же учится понимать синтаксис, семантику и контекст слов, что позволяет ей в дальнейшем эффективно адаптироваться к узким задачам. Основными методами предобучения являются \acrshort{mlm}\cite{sinha2021maskedlanguagemodelingdistributional} и \acrshort{rtd}.
\par Задача извлечения признаков заключается в автоматическом преобразовании исходных данных в компактные числовые векторы, которые сохраняют ключевые характеристики данных и нужны для дальнейшего анализа. 

\end{document}