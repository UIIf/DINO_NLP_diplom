\documentclass[../part_1.tex]{subfiles}

\begin{document}
\subsection{Обработка естественного языка}
\par Обработка естественного языка -- направление искусственного интеллекта, объединяющее лингвистику и компьютерные науки для анализа, понимания и генерации человеческого языка. Современные \acrshort{nlp}-системы способны обрабатывать текст не только на уровне слов, но и понимать контекст, иронию и даже культурные отсылки.
\par В задачах обработки естественного языка входные данные по своей природе не имеют жестких ограничений по длине и структуре. Поэтому для эффективной обработки таких данных требуются архитектуры способные учитывать произвольные зависимости между элементами последовательности. 
% #TODO ФИКС ЧЕЛОВЕКОЧИТАЕМОСТЬ
\par Чтобы представить последовательность текста в виде последовательности чисел используют токенизаторы. Токенизатор -- инструмент, который разбивает текст на отдельные элементы. Элементы могут быть как словами, так и отдельными символами или последовательностями байтов. Этот процесс трансформировать человеко понятный текст в машинно читаемые токены. В современных токенизаторах токен может описывать самые часто встречающиеся слова, отдельные части слов(корни, суффиксы, окончания) так и отдельные символы. 
% #TODO 
% #TODO Процесс обучения эмбедингов производится вместе с обучением модели во время решения других задачь
\par Для того чтобы легче менять значимость токена, используется слой Embedding. В результате работы данного слоя каждому токену сопоставляется вектор, при этом в процессе обучения эти векторы изменяются так, чтобы передавать смысловую нагрузку токенов. % #TODO ПЕРЕПИСАТЬ  В процессе работы модели последовательность токенов преобразуется в последовательность из строк.
\par Для обучения векторов embedding делают предобучение модели. Предобучение -- процесс первоначального обучения нейросети на большом объеме неразмеченных данных с использованием методов \acrfull{ssl}. На этом этапе модель так же учится понимать синтаксис, семантику и контекст слов, что позволяет ей в дальнейшем эффективно адаптироваться к узким задачам. Основными методами предобучения являются \acrshort{mlm}\cite{sinha2021maskedlanguagemodelingdistributional} и \acrshort{rtd}.
% #TODO ГЛАВА ПРО ДООБУЧЕНИЕ ПРО ЗАДАЧУ ИЗВЛЕЧЕНИЯ ПРИЗНАКОВ, КОТОРАЯ ПРЕОБРАЗУЕТ ТЕКСТ В ХОРОШИЕ ВЕКТОРЫ
\par После предобучения модель дообучают на меньших размеченных датасетах под конкретную задачу. Например, дообученная сеть может быть использован для классификации спама или генерации ответов в чат боте. Такой подходи экономит время и улучшает качество модели, поскольку предобучение закладывает в модели базовое понимание языка. Таким образом даже маленький набор целевых данных может демонстрировать высокую точность.
\end{document}